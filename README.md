
# Отчёт по проекту: Sound Classification

## 1. Введение

В рамках проекта реализована система классификации звуков, способная различать различные звуковые классы, такие как «Пылесос», «Сирена», «Двигатель (низкие частоты)», «Речь» и другие. Для обучения модели использовалось 30 000 звуковых файлов (wav) для тренировочного набора и 3000 файлов для тестирования, при этом классы были сбалансированы.

## 2. Постановка задачи

**Цель проекта:**  
Разработать модель, способную классифицировать аудиосигналы по предопределённым классам звуков. Основными задачами являются:

- Подготовка и предварительная обработка аудиоданных.
- Реализация архитектуры модели.
- Обучение модели и оценка её эффективности на тестовом наборе.
- Проведение экспериментов, анализ результатов и формулировка выводов.

## 3. Данные

**Тренировочный набор:** 30 000 wav файлов  
**Тестовый набор:** 3000 wav файлов  
**Классы:** Среди классов присутствуют такие, как Vacuum cleaner, Siren, Heavy engine (low frequency), Speech и другие. Классы сбалансированы, что позволяет объективно оценивать производительность модели.

## 4. Предварительная обработка и аугментация данных

Для преобразования аудио сигналов(они имеют фиксированную длину,поэтому фиксированный time_mask_param оправдан) использовались следующие трансформации:

- **MelSpectrogram:** Преобразование сигнала в мел-спектрограмму с числом меловых полос FEATS = 80.
- **AmplitudeToDB:** Преобразование амплитуд в децибелы.
- **Аугментации (только для тренировочного набора):**
  - *FrequencyMasking:* Случайное «маскирование» по частоте (параметр freq_mask_param=15).
  - *TimeMasking:* Случайное «маскирование» по времени (параметр time_mask_param=100).

Применяемые трансформации позволяют модели стать более устойчивой к вариациям звуковых сигналов.

## 5. Архитектура модели и методология

Для решения задачи классификации одной из главных задач было построить маленькую модель(ориентировался на 100 тысяч параметров), были протестированы модели: CNN 1D, CNN 2D, RCNN(LSTM + CNN), Transformer и миксы архитектур. В ходе эксперементов было принято решение выбрать оптимальное по соотношению числа параметров/вычислительной сложности/качества - ResNet блоки + Transformer Only Encoder, ResNet блоки выделяют локальные признаки из лог-мел-спектрограммы, а трансформер анализирует долгосрочные временные зависимости в аудиофичах.

### 5.1. Сверточные блоки  
Использованы *ResidualBlock*-блоки, которые обеспечивают эффективное извлечение локальных признаков из лог-мел-спектрограмм. Два последовательных сверточных блока позволяют снизить размерность и повысить представительность признаков.

### 5.2. Трансформер  
После сверточных слоёв применяется модуль трансформера:
- **PositionalEncoding:** Добавление информации о позиции в последовательности признаков(использованы самые простые позиционные эмбэдинги - Sinusoidal Positional Encoding, в теории можно было использовать что-то хитрее, например обучаемые, или, в идеале, RoPE).
- **TransformerEncoder:** Однослойный энкодер, который помогает моделировать зависимости во временной последовательности.
- **CLS-токен:** Используется для агрегирования информации по всей последовательности.

### 5.3. Классификатор  
Заключительный классификатор включает:
- Слой нормализации (LayerNorm).
- Блок SwiGLU – модифицированный вариант активации, сочетающий линейные преобразования и нелинейную функцию (сигмоиду).
- Dropout для регуляризации.
- Линейный слой, отображающий представление в пространство классов.

Общее число параметров модели составляет **124 510**(c конкретными параметрами модели можно ознакомиться в Jupyter Notebook).

## 6. Экспериментальная установка

### 6.1. Обучение  
Модель обучалась с использованием оптимизатора **AdamW** с начальной скоростью обучения 0.001 и weight_decay=1e-4. Также использовался scheduler **CosineAnnealingLR** с T_max=50.

### 6.2. Функции оценки  
Для оценки качества модели использовалась функция **cross-entropy loss**. Помимо этого, была реализована функция **balanced_accuracy**(с формулой и реализацией можно ознакомиться в Jupyter Notebook), которая вычисляет точность по классам и усредняет результаты, что особенно важно при наличии сбалансированных классов.

### 6.3. Процесс обучения  
Обучение проводилось в течение 50 эпох. Для каждой эпохи вычислялись показатели для обучающей и тестовой выборок. Визуализация показателей (потерь и точности) по эпохам позволяла следить за динамикой обучения.

## 7. Результаты экспериментов

По окончании обучения (50 эпох) были получены следующие результаты:

- **Обучающая выборка:**  
  - Потеря (CE Loss): ≈1.748  
  - Точность: ≈45.7%
  
- **Тестовая выборка:**  
  - Потеря (CE Loss): ≈1.800  
  - Точность: ≈46.4%

Эти результаты указывают на то, что модель достигла схожих показателей как на тренировочных, так и на тестовых данных, что говорит о минимальном переобучении. Однако итоговая точность около 46% оставляет пространство для улучшений.

## 8. Обсуждение и перспективы

### 8.1. Анализ текущих результатов  
Модель показала **Balanced Accuracy** 46%, что, с одной стороны, может показаться недостаточным результатом, с другой стороны, хоть мы и использовали архитектуру предназначенную для классификации изображений, а в них мы привыкли - точность < 80% - плохой результат, стоит понимать, что модель получилась небольшой , и в целом , без потери качества, поперебирая гиперпараметры модели, мы можем добиться и размера < 100_000 параметров, а также то, что классификация изображений задача даже для нейронной сети более простая, чем классификация звука(хотя по факту мы классифицируем изображение), в этом мы можем убедиться, зайдя на paperswithcode и сравнить SOTA результаты на классификации изображений и звука, хотя не отрицаю, что можно было даже при таком количестве параметров достичь качества ~ 50%-55%.C графиками можно ознакомиться ниже:


![Loss](https://github.com/Cowwwperwood/SoundClassification/blob/main/loss.png)


![Acc](https://github.com/Cowwwperwood/SoundClassification/blob/main/acc.png)




### 8.2. Возможные пути улучшения  
- **Увеличение сложности модели:** Добавление дополнительных слоёв трансформера или сверточных блоков,при этом нужно более тщательно подходить к вопросам переобучения.
- **Тонкая настройка гиперпараметров:** Изменение скорости обучения, увеличение количества эпох, подбор оптимальных параметров аугментаций.
- **Использование предобученных моделей:** Применение transfer learning с предобученными аудио моделями может значительно повысить качество классификации.

## 9. Заключение

В данном проекте была реализована система для классификации звуков, объединяющая сверточные сети и трансформер. Реализованный подход позволил обработать аудиосигналы и извлечь из них информативные признаки, однако достигнутая точность (~46%) свидетельствует о том, что дальнейшие исследования и оптимизация модели необходимы. Перспективными направлениями являются улучшение архитектуры, тонкая настройка гиперпараметров и применение методов transfer learning.
